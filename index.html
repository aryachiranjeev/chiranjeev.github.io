<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chiranjeev</title>
  
  <meta name="author" content="Ishan Dave">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">

	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chiranjeev</name>
              </p>
              <p style="font-size:15px">I am a Ph.D. student at Image Analysis and Biometrics (IAB) Lab, Indian Institute of Technology Jodhpur(IITJ).
              </p>
              <p style="text-align:center">
                <a href="mailto:chiranjeev.1@iitj.ac.in">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cibl9gMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/aryachiranjeev">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chiranjeev-chiranjeev-566723175/">LinkedIn</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/chiranjeev_profilepic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chiranjeev_profilepic.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Updates <strong><font color="red">Looking for fulltime jobs!</font></strong> </h2>
                  <p>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'24:</span> <strong>2 First author papers</strong> accepted at <strong>ECCV 2024</strong><br>
<!-- 			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'24:</span> Started internship at Apple, Cupertino, CA<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Dec'23:</span> A <strong>First author paper</strong> "No More Shortcuts" accepted to <strong>AAAI 2024</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'23:</span> A <strong>First author paper</strong> "Event-TransAct" accepted at IROS 2023 üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jul'23:</span> A paper "TeD-SPAD" accepted at ICCV 2023<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'23:</span> Started summer internship at Adobe, San Jose, CA<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'23:</span> A <strong>First author paper</strong> "TimeBalance" accepted to <strong>CVPR 2023</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan'23:</span> A paper "TransVisDrone" accepted at ICRA 2023<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> May'22:</span> Started summer internship at Adobe, USA (remote- Florida)<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'22:</span> A <strong>First author paper</strong> "TCLR" accepted to CVIU 2022 üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Mar'22:</span> A <strong>First author paper</strong> "SPAct" accepted to <strong>CVPR 2022</strong> üí•<br>
			  <span style="font-family: 'Courier New', Courier, monospace; color: #696969;">&nbsp;<i class="fa fa-share-alt" style="font-size:12px"></i> Jan'21:</span> Our Gabriella paper has been awarded the best scientific paper award at ICPR 2020<br> -->
<!--                     &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020) ü•á<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard ü•á<br> -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
	
<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>
        
	<table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top; text-align:center">
		<img src='images/apple_logo.png' style="width:50%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>PhD AI/ML Intern</strong>
                <br> Apple Inc., Cupertino, California, USA. May 2024- Aug 2024
	        <p></p>
		<ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced stable diffusion models for image editing by leveraging vision-language multimodal foundation models.</li>
		    <li>Trained diffusion models on a large-scale, high-resolution dataset of 10M samples.</li>
		    <li>Reproduced and outperformed state-of-the-art image editing methods using a novel approach, achieving superior results.</li>
		</ul>	      
              </td>
		<p></p>
              </td>
            </tr>
          </tr>
          </table>
	
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist/ Engineer Intern</strong>
                <br> Adobe Inc., San Jose, California, USA. May 2023- Nov 2023
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni, </a>
                <a href="https://fabiancaba.com/">Fabian Caba</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Enhanced the fine-grained capabilities of existing video retrieval methods.</li>
		    <li>Worked on large-scale video galleries with millions of samples.</li>
		    <li>Filed a patent and had a paper accepted at ECCV 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>
	 <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
               <td style="padding:10px;width:25%;vertical-align:top">
		<img src='images/Adobe-logo.png' style="width:100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Scientist Intern</strong>
                <br> Adobe Inc., Remote, USA. May 2022 - Nov 2022
                <br> Host: <a href="https://sjenni.github.io/">Simon Jenni</a>
                <p></p>
                <ul style="margin: 0; padding-left: 20px; line-height: 1.2;">
		    <li>Developed a novel self-supervised video representation framework by reformulating temporal self-supervision as frame-level recognition tasks and introducing an effective augmentation strategy to mitigate shortcuts.</li>
		    <li>Achieved state-of-the-art performance on 10 video understanding benchmarks across linear classification (Kinetics400, HVU, SSv2, Charades), video retrieval (UCF101, HMDB51), and temporal correspondence (CASIA-B).</li>
		    <li>Published a paper at AAAI 2024.</li>
		</ul>	      
              </td>
            </tr>
          </tr>
          </table>

          -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have a broad research interest in Computer Vision and Deep Learning. My primary research focuses on <strong>Face Recognition</strong>: Facial Recogition for Securtiy and Social Good.
<!-- 		      My recent research also includes enhancing the fine-grained video understanding of the large foundational models and improving multi-modal generative AI for image editing applications. I have also worked on various robotics-related vision tasks like event-camera-based action recognition and drone-to-drone detections from videos.  -->
                <br> Below mentioned are some of my research publications.

              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/hyperspacex_eccv24.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
<!-- 		<a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/avr_eccv24_dave.pdf"> -->
		    <papertitle>HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions</papertitle>
		</a>
		<br>
		<strong>Chiranjeev Chiranjeev</strong>, 
		Muskan Dosi, 
		KArtik Thakral, 
		Mayank Vatsa,
    Richa Singh.
		<br>
		<em>The European Conference on Computer Vision (<strong>ECCV</strong>) </em>, 2024
    <br>
		<a href="http://arxiv.org/abs/2408.02494">[paper]</a> <a href="https://github.com/IAB-IITJ/HyperSpaceX">[code]</a> <a href="https://github.com/IAB-IITJ/HyperSpaceX">[video]</a> 
		<p></p>
<!-- 		<p>
			Temporal video alignment synchronizes key events like object interactions or action phase transitions in two videos, benefiting video editing, processing, and understanding tasks. Existing methods assume a given video pair, limiting applicability. We redefine this as a search problem, introducing Alignable Video Retrieval (AVR), which identifies and synchronizes well-alignable videos from a large collection. Key contributions include DRAQ, a video alignability indicator, and a generalizable frame-level video feature design.

		</p> -->
	     </td>
	</tr>

          
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">	
		<td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
			            <img src='images/superresolution_ijcb24.png' width="180">
			        </div>
			    </td>
			    <td style="padding:20px;width:75%;vertical-align:middle">
<!-- 		        <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/finepsuedo_eccv24_dave.pdf"> -->
		            <papertitle>Does Face Super-Resolution Really Advances Low-Resolution Face Recognition?</papertitle>
		        </a>
		        <br>
		        Muskan Dosi*, 
		        Udaybhan Rathoore*, 
		        <strong>Chiranjeev Chiranjeev</strong>,
            Akshay Agarwal,
            Richa Singh,
            Mayank Vatsa.
		        <br>
		        <em>IEEE International Joint Conference on Biometrics (<strong>IJCB</strong>) </em>, 2024
		        <br>
			<a href="">[paper]</a> 
		   	<p></p>
<!-- 			<p>
				We introduce Alignability-Verification-based Metric learning for semi-supervised fine-grained action recognition. Using dynamic time warping (DTW) for action-phase-aware comparison, our learnable alignability score refines pseudo-labels of the video encoder. Our framework, FinePseudo, outperforms prior methods on fine-grained action recognition datasets. Additionally, it demonstrates robustness in handling novel unlabeled classes in open-world setups.
			</p> -->
		    	</td>
	</tr>

		
<!-- 	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0"> -->
		
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/dlord_tbiom24.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
<!--         <a href="https://arxiv.org/pdf/2312.13008.pdf"> -->
            <papertitle>D-LORD: DYSL-AI Database for Low-Resolution Disguised Face Recognition</papertitle>
        </a>
        <br>
        Sunny Manchanda*,
        Kaushik Bhagwatkar*,    
        Kavita Balutia*,    
        Shivang Agarwal*,      
        Jyoti Chaudhary*,
        Muskan Dosi*
        <strong>Chiranjeev Chiranjeev</strong>*, 
        Mayank Vatsa
        Richa Singh.
        <br>
        <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (<strong>T-BIOM: 5.81</strong>) </em>, 2024
        <br>
	<a href="https://ieeexplore.ieee.org/document/10224654">[paper]</a> &nbsp/&nbsp <a href="https://github.com/IAB-IITJ/D-LORD">[dataset]</a>
   	<p></p>
<!-- 	<p>
		We propose a Domain Adaptive Contrastive objective to bridge the gap between High and Low Cost Microscopes. On the publicly available large-scale M5 dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides a 21√ó speed-up during inference, and requires only half as many learnable parameters as the prior methods. -->
<!-- 	</p> -->
    	</td>
<!-- 	</tr> -->



		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/ugldface_ijcb23.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
<!--         <a href="https://arxiv.org/pdf/2312.13008.pdf"> -->
            <papertitle>UG-LDFace: Unified and Generalized Framework for Long-Range Disguised Face Recognition</papertitle>
        </a>
        <br>
        Muskan Dosi,
        <strong>Chiranjeev Chiranjeev</strong>, 
        Richa Singh,  
        Mayank Vatsa.
        <br>
        <em>IEEE International Joint Conference on Biometrics (<strong>IJCB</strong>) </em>, 2023
        <br>
	<a href="https://ieeexplore.ieee.org/document/10449234">[paper]</a>
   	<p></p>
<!-- 	<p>
		We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. Our extensive experiments show state-of-the-art performance across 10 video understanding datasets, illustrating the generalization ability and robustness of our learned video representations.
	</p> -->
    	</td>
	</tr>

		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/segdgd_jstsp23.png' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
<!-- 		<a href="LINK_TO_PAPER"> -->
		    <papertitle>Seg-DGDNet: Segmentation based Disguise Guided Dropout Network for Low Resolution Face Recognition</papertitle>
		</a>
		<br>
    Muskan Dosi*
    <strong>Chiranjeev Chiranjeev</strong>*, 
    Shivang Agarwal*,      
    Jyoti Chaudhary*,
		Sunny Manchanda*,
    Kavita Balutia*,
    Kaushik Bhagwatkar*,        
    Mayank Vatsa
    Richa Singh.
		<br>
		<em>IEEE Journal of Selected Topics in Signal Processing (<strong>IF: 7.695</strong>)</em>, 2023
		<br>
		<a href="https://ieeexplore.ieee.org/document/10163869">[paper]</a>
		    <p></p>
<!-- 			<p>
			    We propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods.
			</p> -->
	    </td>
	</tr>

<!-- 	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/eventtransact_iros2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="LINK_TO_PAPER">
	            <papertitle>EventTransAct: A Video Transformer-based Framework for Event-camera Based Action Recognition</papertitle>
	        </a>
	        <br>
	        Tristan de Blegiers*, 
	        <strong>Ishan Rajendrakumar Dave*</strong>, 
	        Adeel Yousaf, 
	        Mubarak Shah.
		<br>
		*= equal contribution
	        <br>
	        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
	        <br>
	        <a href="LINK_TO_PAPER">paper</a> &nbsp/&nbsp <a href="https://github.com/tristandb8/EventTransAct">code</a> &nbsp/&nbsp <a href="https://joefioresi718.github.io/TeD-SPAD_webpage/">project page</a>
	    	<p></p>
		<p>
		   We propose a video transformer-based framework for event-camera based action recognition, which leverages event-contrastive loss and augmentations to adapt the network to event data. Our method achieved state-of-the-art results on N-EPIC Kitchens dataset and competitive results on the standard DVS Gesture recognition dataset, while requiring less computation time compared to competitive prior approaches. 
		</p>
	    </td>
	</tr>

		
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/timebalance_cvpr2023.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf">
            <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
        </a>
        <br>
        <strong>Ishan Rajendrakumar Dave</strong>, 
        Mamshad Nayeem Rizve, 
        Chen Chen, 
        Mubarak Shah.
        <br>
        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2303.16268.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/TimeBalance">code</a> &nbsp/&nbsp <a href="https://daveishan.github.io/timebalance_webpage/">project page</a>
   	<p></p>
	<p>
		We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. State-of-the-art results on Kinetics400, UCF101, HMDB51.

	</p>
    	</td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/transvisdrone_icra23.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/10161433">
	            <papertitle>Transvisdrone: Spatio-temporal Transformer for Vision-based Drone-to-drone Detection in Aerial Videos</papertitle>
	        </a>
	        <br>
	        Tushar Sangam, 
	        <strong>Ishan Rajendrakumar Dave</strong>,
	        Waqas Sultani, 
	        Mubarak Shah.
	        <br>
	        <em>2023 IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023
	        <br>
	        <a href="https://arxiv.org/pdf/2210.08423.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/tusharsangam/TransVisDrone">code</a> &nbsp/&nbsp <a href="https://tusharsangam.github.io/TransVisDrone-project-page/">project page</a>
			<p></p>
			<p>
			    We propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion.
			</p>
	    </td>
	</tr>

	

	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/spact_cvpr2022.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dave_SPAct_Self-Supervised_Privacy_Preservation_for_Action_Recognition_CVPR_2022_paper.html">
	            <papertitle>SPAct: Self-supervised Privacy Preservation for Action Recognition</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Rajendrakumar Dave</strong>, 
	        Chen Chen, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
	        <br>
	        <a href="https://arxiv.org/pdf/2203.15205.pdf">paper</a> &nbsp/&nbsp <a href="https://github.com/DAVEISHAN/SPAct">code</a>
		    <p></p>
			<p>
			    For the first time, we present a novel training framework that removes privacy information from input video in a self-supervised manner without requiring privacy labels. We train our framework using a minimax optimization strategy to minimize the action recognition cost function and maximize the privacy cost function through a contrastive self-supervised loss. 
			</p>
	    </td>
	</tr>

	

	
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()" bgcolor="#ffffd0">
	    <td style="padding:5px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/tclr_cviu2022.png' width="180">
	        </div>
	        <script type="text/javascript">
	            function mira_start() {
	                document.getElementById('mira_image').style.opacity = "1";
	            }
	
	            function mira_stop() {
	                document.getElementById('mira_image').style.opacity = "0";
	            }
	            mira_stop()
	        </script>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">
	            <papertitle>TCLR: Temporal Contrastive Learning for Video Representation</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>,
	        Rohit Gupta, 
	        Mamshad Nayeem Rizve, 
	        Mubarak Shah.
	        <br>
	        <em>Computer Vision and Image Understanding (<strong>CVIU</strong>)</em>, 2022
	        <br>
	       <em><strong><font color="red">(100+ citations, Among the top-10 most downloaded papers in CVIU)</font></strong></em>
               <br>
	        <a href="https://arxiv.org/pdf/2101.07974.pdf">paper</a> &nbsp/&nbsp
	        <a href="https://github.com/DAVEISHAN/TCLR">code</a>
	        <p></p>
	        <p>
	            We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.
	        </p>
	    </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriellav2_wacv22.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">
	            <papertitle>Gabriellav2: Towards Better Generalization in Surveillance Videos for Action Detection</papertitle>
	        </a>
	        <br>
	        <strong>Ishan Dave</strong>, 
	        Zacchaeus Scheffer, 
	        Akash Kumar, 
	        Sarah Shiraz, 
	        Yogesh Singh Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2022
	        <br>
	        <a href="https://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf">paper</a>
	        <p></p>
	        <p>
	            We propose a realtime, online, action detection system which can generalize robustly on any unknown facility surveillance videos. We tackle the
challenging nature of action classification problem in various aspects like handling the class-imbalance training using PLM method and learning multi-label action correlations using LSEP loss. In order to improve the computational efficiency of the system, we utilize knowledge distillation.
	        </p>
	    </td>
	</tr>	
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
	        <div class="one">
	            <img src='images/gabriella_icpr2020.png' width="180">
	        </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	        <a href="https://ieeexplore.ieee.org/document/9412791">
	            <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
	        </a>
	        <br>
	        Mamshad Nayeem Rizve, 
	        Ugur Demir, 
	        Praveen Tirupattur, 
	        Aayush Jung Rana, 
	        Kevin Duarte, 
	        <strong>Ishan R Dave</strong>, 
	        Yogesh S Rawat, 
	        Mubarak Shah.
	        <br>
	        <em>25th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2021 <em><strong><font color="red">(Best Paper Award)</font></strong></em>
	        <br>
	        <a href="https://arxiv.org/abs/2004.11475">paper</a> 
		<p></p>
	        <p>
	            Gabriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.
	        </p>    
	    </td>
	</tr>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Patent</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
	    <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
		    <img src='images/spact_patent2023.jpg' width="180">
		</div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="LINK_TO_PAPER">
		    <papertitle>Self-Supervised Privacy Preservation Action Recognition System</papertitle>
		</a>
		<br>
		<strong>Ishan Rajendrakumar Dave</strong>, 
		Chen Chen,
		Mubarak Shah, 	
		<br>
		<em> The University of Central Florida. Invention Track Code: 2023-019. (Status: Approved) </em>, 2023
		<br>
		<a href="https://ucf.flintbox.com/technologies/781d7a2d-1f89-41db-911c-5d4ecf26af20">Tech Sheet</a>
		    
	    </td>
	</tr> -->


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Achievements</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	  <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png" width="50%" alt="Trophy Image"></td>
				
          <td width="75%" valign="center">
		<!-- Outstanding Reviewer 2024 -->
		    <div>
<!-- 		        <a href="https://twitter.com/CVPR/status/1793616950314369239/photo/1"> -->
<!-- 		            <strong>ParichayAI Tool</strong> 2024 -  -->
		            Presented ParichayAI: Face Recognition for Surveillance and Social Good" at IInventive-2024.
		        </a>
		    </div>
		    <br>
		  
		  <div>
<!-- 		        <a href="https://www.crcv.ucf.edu/2023/09/21/congratulations-to-crcv-ph-d-students-jyoti-kini-ishan-dave-and-undergraduate-student-sarah-fleischer/"> -->
		            Collaborated with Ministry of Communications (India) for Victim Identification via Face Recognition in Balasore Train Accident, India.
		        </a>
		    </div>
		    <br>
		  
    		<!-- 1st Prize 2023 -->
		  <div>
<!-- 		        <a href="https://www.crcv.ucf.edu/2023/09/21/congratulations-to-crcv-ph-d-students-jyoti-kini-ishan-dave-and-undergraduate-student-sarah-fleischer/"> -->
		            Received prestigious Prime Minister Research Fellowship (PMRF).
		        </a>
		    </div>
		    <br>
		  
		    <div>
<!-- 		        <a href="https://www.crcv.ucf.edu/2023/09/21/congratulations-to-crcv-ph-d-students-jyoti-kini-ishan-dave-and-undergraduate-student-sarah-fleischer/"> -->
		            Volunteered in IEEE Conference on Face and Gesture Recognition, 2021.
		        </a>
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2022 -->
		    <div>
<!-- 		        <a href="https://activity-net.org/challenges/2022/challenge.html"> -->
<!-- 		            <strong>2<sup>nd</sup> place,</strong> 2022 -  -->
		            Qualified Gate 2021 with 96.2 percentile.
<!-- 		        </a> -->
		    </div>
		    <br>
		
		    <!-- 2nd Prize 2021 -->
<!-- 		    <div>
		        <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
		            <strong>2<sup>nd</sup> place,</strong> 2021 - 
		            NIST TRECVID ActEV: Activities in Extended Video
		        </a>
		    </div>
		    <br>
		
		    <!-- 1st Prize & Jury Prize 2021 -->
<!-- 		    <div>
		        <a href="https://vipriors.github.io/2021/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place & Jury Prize,</strong> 2021 - 
		            VI-Priors Action Recognition Challenge (<strong>ICCV</strong>)
		        </a>
		    </div>
		    <br> -->
		
		    <!-- 1st Prize 2021 PMiss@0.02tfa -->
<!-- 		    <div> -->
<!-- 		        <a href="#"> -->
<!-- 		            <strong>1<sup>st</sup> place,</strong> 2021 - 
		            PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>) -->
<!-- 		        </a> -->
<!-- 		    </div>
		    <br> -->
		
		    <!-- 1st Prize 2020 VI-Priors -->
<!-- 		    <div>
		        <a href="https://vipriors.github.io/2020/challenges/#action-recognition">
		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            VI-Priors Action Recognition Challenge (<strong>ECCV</strong>)
		        </a>
		    </div>
		    <br> -->
		
		    <!-- 1st Prize 2020 PMiss and nAUDC -->
<!-- 		    <div> -->
<!-- 		        <a href="#"> -->
<!-- 		            <strong>1<sup>st</sup> place,</strong> 2020 - 
		            PMiss and nAUDC, ActivityNet ActEV SDL (<strong>CVPR</strong>) -->
<!-- 		        </a> -->
<!-- 		    </div>
		    <br> -->
		
		    <!-- 2nd Prize 2020 TRECVID -->
<!-- 		    <div> -->
<!-- 		        <a href="#"> -->
<!-- 		            <strong>2<sup>nd</sup> place,</strong> 2020 - 
		            TRECVID ActEV: Activities in Extended Video -->
<!-- 		        </a> -->
<!-- 		    </div>
		    <br> -->
		
		    <!-- ORCGS Doctoral Fellowship 2019-2020 -->
<!-- 		    <div> -->
<!-- 		        <a href="#"> -->
<!-- 		            <strong>ORCGS Doctoral Fellowship,</strong> 2019-2020 -->
<!-- 		        </a> -->
<!-- 		    </div> -->
<!-- 		    <br> -->
		
		    <!-- Top 0.5% 2013 -->
<!-- 		    <div>
		        <a href="#">
		            <strong>Top 0.5%,</strong> 2013 - 
		            Joint Engineering Entrance-Mains exam, India
		        </a>
		    </div>  -->
		</td>
		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Reviewing experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
		<p>ECCV 2024 </p><br>	
		<p> NeurIPS (Dataset Track) 2022, 2023, 2024 </p><br>	
		<p>Pattern Recognition Journal </p><br>	
<!-- 		<a href="https://eccv2024.ecva.net/">Reviewer, ECCV 2024</a><br>	
		<a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2024, 2023, 2022</a><br>
		<a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a><br>
		<a href="https://eccv2024.ecva.net/">Reviewer, WACV 2025, 2024</a><br>	
		<a href="https://2024.ieee-icra.org/">Reviewer, ICRA 2024</a><br>	
		<a href="https://iros2024-abudhabi.org/">Reviewer, IROS 2024</a><br>	
	      	<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction on Image Processing</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">Reviewer, IEEE Transactions on Multimedia</a><br>
	        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">Reviewer, IEEE Transactions on Circuits and Systems for Video Technology</a><br>
		<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">Reviewer, IEEE Transactions on Neural Networks and Learning Systems</a><br>    
	        <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">Reviewer, Computer Vision and Image Understanding</a><br>
		<a href="https://www.sciencedirect.com/journal/pattern-recognition">Reviewer, Pattern Recognition</a><br>
		<a href="https://www.sciencedirect.com/journal/expert-systems-with-applications">Reviewer, Expert Systems with Applications</a><br>
		<a href="https://www.sciencedirect.com/journal/image-and-vision-computing">Reviewer, Image and Vision Computing</a><br>
		<a href="https://link.springer.com/journal/11554">Reviewer, Journal of Real-Time Image Processing</a><br>
		<a href="https://link.springer.com/journal/11042">Reviewer, Multimedia Tools and Applications</a><br>

              <br>
              <br> -->
                 
            </td>
          </tr>	

<!-- 		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Mentor in NSF-REU</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png" width="50%" alt="NSF Image"></td>
            <td width="75%" valign="center">
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2022/">Kevin Chung, REU 2022</a>
	        <br>

		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Ethan Thomas, REU 2021</a>
		<br>
		<a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Kali Carter, REU 2020</a>

              <br>
              <br>
		    
            </td>
          </tr>					
					
        </tbody></table> -->
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
